{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash-joshi/twitter-sentiment/blob/master/twitter_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "noV4F3w_CKHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "0fd85601-e970-4c5e-8d85-323be09ab601"
      },
      "cell_type": "code",
      "source": [
        "!wget -c https://raw.githubusercontent.com/crwong/cs224u-project/master/data/sentiment/training.1600000.processed.noemoticon.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-29 05:15:49--  https://raw.githubusercontent.com/crwong/cs224u-project/master/data/sentiment/training.1600000.processed.noemoticon.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2989873 (2.9M) [text/plain]\n",
            "Saving to: ‘training.1600000.processed.noemoticon.csv’\n",
            "\n",
            "training.1600000.pr 100%[===================>]   2.85M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-01-29 05:15:54 (42.4 MB/s) - ‘training.1600000.processed.noemoticon.csv’ saved [2989873/2989873]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BpAq-CpFB3nS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "da33fa05-ff20-4496-f1a1-46c6bf04a967"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "def import_tweets(filename, header = None):\n",
        "\t#import data from csv file via pandas library\n",
        "\ttweet_dataset = pd.read_csv(filename, encoding = 'latin-1', header = header)\n",
        "\t#the column names are based on sentiment140 dataset provided on kaggle\n",
        "\ttweet_dataset.columns = ['sentiment','id','date','flag','user','text']\n",
        "\t#delete 3 columns: flags,id,user, as they are not required for analysis\n",
        "\tfor i in ['flag','id','user','date']: del tweet_dataset[i] # or tweet_dataset = tweet_dataset.drop([\"id\",\"user\",\"date\",\"user\"], axis = 1)\n",
        "\t#in sentiment140 dataset, positive = 4, negative = 0; So we change positive to 1\n",
        "\ttweet_dataset.sentiment = tweet_dataset.sentiment.replace(4,1)\n",
        "\treturn tweet_dataset\n",
        "\t\n",
        "def preprocess_tweet(tweet):\n",
        "\t#Preprocess the text in a single tweet\n",
        "\t#arguments: tweet = a single tweet in form of string \n",
        "\t#convert the tweet to lower case\n",
        "\ttweet.lower()\n",
        "\t#convert all urls to sting \"URL\"\n",
        "\ttweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
        "\t#convert all @username to \"AT_USER\"\n",
        "\ttweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
        "\t#correct all multiple white spaces to a single white space\n",
        "\ttweet = re.sub('[\\s]+', ' ', tweet)\n",
        "\t#convert \"#topic\" to just \"topic\"\n",
        "\ttweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "\treturn tweet\n",
        "\n",
        "def feature_extraction(data, method = \"tfidf\"):\n",
        "\t#arguments: data = all the tweets in the form of array, method = type of feature extracter\n",
        "\t#methods of feature extractions: \"tfidf\" and \"doc2vec\"\n",
        "\tif method == \"tfidf\":\n",
        "\t\tfrom sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\t\ttfv=TfidfVectorizer(sublinear_tf=True, stop_words = \"english\") # we need to give proper stopwords list for better performance\n",
        "\t\tfeatures=tfv.fit_transform(data)\n",
        "\telse:\n",
        "\t\treturn \"Incorrect inputs\"\n",
        "\treturn features\n",
        "\n",
        "#apply the preprocess function for all the tweets in the dataset\n",
        "tweet_dataset = import_tweets(\"training.1600000.processed.noemoticon.csv\")\n",
        "tweet_dataset['text'] = tweet_dataset['text'].apply(preprocess_tweet)\n",
        "data = np.array(tweet_dataset.text)\n",
        "label = np.array(tweet_dataset.sentiment)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train , X_test , ytrain , ytest = train_test_split(data,label,test_size=0.2)\n",
        "\n",
        "features = feature_extraction(data, method = \"tfidf\") #1600000x288571 sparse matrix of type 'numpy.float64\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "#fit model to data\n",
        "model.fit(features, label)\n",
        "#make prediction on the same (train) data\n",
        "\n",
        "predictions = model.predict(features)\n",
        "\n",
        "print(accuracy_score(label,predictions))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.878\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}